å¤ªå¥½äº†ï¼ä½ å·²æœ‰ Python ç¯å¢ƒå’Œæœ¬åœ° LLMï¼ˆé€šè¿‡ `llama.cpp` + Python å°è£…ï¼Œå¦‚ `llama-cpp-python`ï¼‰ï¼Œè¿™éå¸¸é€‚åˆæ„å»ºéšç§å®‰å…¨ã€å¯è‡ªä¸»æ¼”åŒ–çš„ Agentic ç³»ç»Ÿã€‚

æˆ‘ä»¬ç°åœ¨è¿›å…¥ **é˜¶æ®µ 1ï¼šåŸºç¡€æ¡†æ¶æ­å»º**ã€‚

---

## âœ… é˜¶æ®µ 1 äº¤ä»˜å†…å®¹æ¸…å•

1. **Agentic Graph YAML Schema (v1.0)**  
2. **è¡¨è¾¾å¼è§£ææ¨¡å—ï¼ˆå®‰å…¨ã€æ”¯æŒ Jinja2 é£æ ¼å˜é‡ï¼‰**  
3. **åŸºç¡€æ‰§è¡Œå¼•æ“ï¼ˆExecutionEngine + èŠ‚ç‚¹ç±»ï¼‰**  
4. **å·¥å…·æ³¨å†Œæœºåˆ¶ + ç¤ºä¾‹å·¥å…·**  
5. **LLM èŠ‚ç‚¹é›†æˆï¼ˆé€‚é… llama-cpp-pythonï¼‰**  
6. **ç¤ºä¾‹ Agentic Graphï¼ˆYAMLï¼‰**  
7. **è¿è¡Œè„šæœ¬ä¸éªŒè¯å‘½ä»¤**

---

## 1ï¸âƒ£ Agentic Graph Schema (v1.0)

æˆ‘ä»¬ä½¿ç”¨ **JSON Schema** æ¥å®šä¹‰åˆæ³•ç»“æ„ï¼Œä¾¿äºåç»­éªŒè¯ã€‚

ğŸ“ æ–‡ä»¶ï¼š`schemas/agentic_graph_v1.json`

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "graph": {
      "type": "object",
      "properties": {
        "name": {"type": "string"},
        "version": {"type": "string", "pattern": "^\\d+\\.\\d+$"}
      },
      "required": ["name", "version"]
    },
    "nodes": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "id": {"type": "string"},
          "type": {"type": "string", "enum": ["start", "end", "set", "llm_call", "tool_call"]},
          "next": {"type": "string"}
        },
        "required": ["id", "type"],
        "allOf": [
          {
            "if": {"properties": {"type": {"const": "set"}}},
            "then": {
              "properties": {
                "assign": {"type": "object"}
              },
              "required": ["assign"]
            }
          },
          {
            "if": {"properties": {"type": {"const": "llm_call"}}},
            "then": {
              "properties": {
                "prompt_template": {"type": "string"},
                "output_key": {"type": "string"}
              },
              "required": ["prompt_template", "output_key"]
            }
          },
          {
            "if": {"properties": {"type": {"const": "tool_call"}}},
            "then": {
              "properties": {
                "tool": {"type": "string"},
                "args": {"type": "object"},
                "output_key": {"type": "string"}
              },
              "required": ["tool", "args", "output_key"]
            }
          }
        ],
        "additionalProperties": false
      },
      "minItems": 1
    }
  },
  "required": ["graph", "nodes"]
}
```

> âœ… æ”¯æŒçš„èŠ‚ç‚¹ç±»å‹ï¼š
> - `start` / `end`ï¼šæµç¨‹èµ·æ­¢
> - `set`ï¼šè®¾ç½®å˜é‡ï¼ˆå¦‚ `assign: { query: "å¤©æ°”å¦‚ä½•" }`ï¼‰
> - `llm_call`ï¼šè°ƒç”¨æœ¬åœ° LLM
> - `tool_call`ï¼šè°ƒç”¨æ³¨å†Œå·¥å…·

---

## 2ï¸âƒ£ è¡¨è¾¾å¼è§£ææ¨¡å—

æˆ‘ä»¬ä½¿ç”¨ **Jinja2 æ¨¡æ¿å¼•æ“**ï¼ˆå®‰å…¨æ¨¡å¼ï¼‰ + **simpleeval**ï¼ˆå¯é€‰ï¼‰æ¥è§£æ `{{...}}` å˜é‡ã€‚

ğŸ“ æ–‡ä»¶ï¼š`core/expr.py`

```python
from jinja2 import Template, StrictUndefined
import re

def render_expression(template_str: str, context: dict) -> str:
    """
    å®‰å…¨æ¸²æŸ“ Jinja2 é£æ ¼æ¨¡æ¿ï¼Œä»…å…è®¸å˜é‡è®¿é—®ï¼Œç¦æ­¢æ‰§è¡Œä»£ç ã€‚
    ç¤ºä¾‹: "{{ input.query }}" -> "åŒ—äº¬å¤©æ°”"
    """
    # ç®€å•é¢„å¤„ç†ï¼šç¡®ä¿æ˜¯å­—ç¬¦ä¸²
    if not isinstance(template_str, str):
        return template_str

    # ä½¿ç”¨ StrictUndefined é¿å…é™é»˜å¤±è´¥
    try:
        template = Template(template_str, undefined=StrictUndefined)
        return template.render(**context)
    except Exception as e:
        raise ValueError(f"Failed to render expression '{template_str}': {e}")

def eval_condition(expr: str, context: dict) -> bool:
    """
    è¯„ä¼°æ¡ä»¶è¡¨è¾¾å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼šä»…æ”¯æŒ ==, !=, and, or, not, True/False, æ•°å­—æ¯”è¾ƒ
    ä¸ºå®‰å…¨èµ·è§ï¼Œæœ¬é˜¶æ®µä»…æ”¯æŒé€šè¿‡ render_expression + å­—ç¬¦ä¸²åˆ¤æ–­
    åç»­å¯ç”¨ simpleeval æ‰©å±•
    """
    rendered = render_expression(expr, context).strip().lower()
    if rendered in ("true", "1", "yes"):
        return True
    elif rendered in ("false", "0", "no", ""):
        return False
    else:
        # å°è¯•ä½œä¸º Python è¡¨è¾¾å¼ï¼ˆè°¨æ…ï¼ï¼‰
        # æš‚ä¸å¯ç”¨ï¼Œé˜¶æ®µ1ä»…æ”¯æŒå¸ƒå°”å­—ç¬¦ä¸²
        raise ValueError(f"Unsupported condition format: '{rendered}' (expected true/false)")
```

> ğŸ”’ å®‰å…¨è¯´æ˜ï¼šJinja2 åœ¨é»˜è®¤é…ç½®ä¸‹æ˜¯å®‰å…¨çš„ï¼ˆæ— ä»£ç æ‰§è¡Œï¼‰ï¼Œ`StrictUndefined` å¯å¸®åŠ©å‘ç°æœªå®šä¹‰å˜é‡ã€‚

---

## 3ï¸âƒ£ åŸºç¡€æ‰§è¡Œå¼•æ“

ğŸ“ æ–‡ä»¶ï¼š`core/engine.py`

```python
import json
import logging
from typing import Dict, Any, Callable
from .expr import render_expression

logger = logging.getLogger(__name__)

class Node:
    def __init__(self, def_dict: Dict[str, Any]):
        self.id = def_dict["id"]
        self.type = def_dict["type"]
        self.next = def_dict.get("next")
        self.raw = def_dict

    def execute(self, context: Dict[str, Any], tools: Dict[str, Callable], llm_client) -> str:
        raise NotImplementedError

class StartNode(Node):
    def execute(self, context, tools, llm_client):
        logger.info("â–¶ Start")
        return self.next

class EndNode(Node):
    def execute(self, context, tools, llm_client):
        logger.info("â¹ End")
        return None

class SetNode(Node):
    def execute(self, context, tools, llm_client):
        assign = self.raw["assign"]
        for key, value in assign.items():
            rendered = render_expression(value, context)
            context[key] = rendered
            logger.info(f"SET {key} = {rendered}")
        return self.next

class LLMCallNode(Node):
    def execute(self, context, tools, llm_client):
        prompt_tmpl = self.raw["prompt_template"]
        output_key = self.raw["output_key"]
        prompt = render_expression(prompt_tmpl, context)
        
        logger.info(f"ğŸ§  LLM Prompt: {prompt[:100]}...")
        response = llm_client(prompt)
        context[output_key] = response
        logger.info(f"ğŸ’¬ LLM Response ({output_key}): {response[:100]}...")
        return self.next

class ToolCallNode(Node):
    def execute(self, context, tools, llm_client):
        tool_name = self.raw["tool"]
        args_tmpl = self.raw["args"]
        output_key = self.raw["output_key"]

        if tool_name not in tools:
            raise ValueError(f"Tool '{tool_name}' not registered")

        # æ¸²æŸ“å‚æ•°
        args = {}
        for k, v in args_tmpl.items():
            args[k] = render_expression(v, context)

        logger.info(f"ğŸ› ï¸ Calling tool: {tool_name} with {args}")
        try:
            result = tools[tool_name](**args)
            context[output_key] = result
            logger.info(f"âœ… Tool result ({output_key}): {str(result)[:100]}...")
        except Exception as e:
            logger.error(f"âŒ Tool failed: {e}")
            raise
        return self.next

NODE_TYPES = {
    "start": StartNode,
    "end": EndNode,
    "set": SetNode,
    "llm_call": LLMCallNode,
    "tool_call": ToolCallNode,
}

class ExecutionEngine:
    def __init__(self, graph_def: dict, tools: dict, llm_client: Callable[[str], str]):
        self.graph_def = graph_def
        self.tools = tools
        self.llm_client = llm_client
        self.context = {}
        self.node_map = {node["id"]: node for node in graph_def["nodes"]}

    def run(self):
        current_id = "start"
        step = 0
        max_steps = 50  # é˜²æ­¢æ— é™å¾ªç¯

        while current_id and step < max_steps:
            node_def = self.node_map.get(current_id)
            if not node_def:
                raise ValueError(f"Node '{current_id}' not found")

            node_type = node_def["type"]
            node_cls = NODE_TYPES.get(node_type)
            if not node_cls:
                raise ValueError(f"Unsupported node type: {node_type}")

            node = node_cls(node_def)
            try:
                next_id = node.execute(self.context, self.tools, self.llm_client)
                current_id = next_id
                step += 1
            except Exception as e:
                logger.exception(f"Execution failed at node '{current_id}': {e}")
                raise

        if step >= max_steps:
            logger.warning("Max steps reached, terminating.")
```

---

## 4ï¸âƒ£ å·¥å…·æ³¨å†Œ + ç¤ºä¾‹å·¥å…·

ğŸ“ æ–‡ä»¶ï¼š`tools/__init__.py`

```python
def web_search(query: str) -> str:
    # æ¨¡æ‹Ÿæœç´¢ï¼ˆåç»­å¯æ›¿æ¢ä¸ºçœŸå® APIï¼‰
    return f"Mock search result for: {query}"

def code_interpreter(code: str) -> str:
    # âš ï¸ ä»…ç”¨äºæ¼”ç¤ºï¼ç”Ÿäº§ç¯å¢ƒéœ€æ²™ç®±
    try:
        # ç®€å•è¡¨è¾¾å¼æ±‚å€¼
        result = eval(code, {"__builtins__": {}}, {})
        return str(result)
    except Exception as e:
        return f"Error: {e}"

# å·¥å…·æ³¨å†Œè¡¨
TOOLS = {
    "web_search": web_search,
    "code_interpreter": code_interpreter,
}
```

> ğŸ”’ æ³¨æ„ï¼š`code_interpreter` åœ¨é˜¶æ®µ 1 ä»…ä¸ºæ¼”ç¤ºï¼Œ**åˆ‡å‹¿ç”¨äºçœŸå®ç”¨æˆ·è¾“å…¥**ã€‚åç»­å¯ç”¨ `asteval` æˆ– Docker æ²™ç®±æ›¿ä»£ã€‚

---

## 5ï¸âƒ£ LLM å®¢æˆ·ç«¯ï¼ˆé€‚é… llama-cpp-pythonï¼‰

å‡è®¾ä½ ä½¿ç”¨ [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python)ï¼š

```bash
pip install llama-cpp-python
```

ğŸ“ æ–‡ä»¶ï¼š`llm/llama_client.py`

```python
from llama_cpp import Llama

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ ¹æ®ä½ çš„è·¯å¾„è°ƒæ•´ï¼‰
LLM = Llama(
    model_path="./models/gguf-model.bin",  # æ›¿æ¢ä¸ºä½ çš„ .gguf æ–‡ä»¶è·¯å¾„
    n_ctx=2048,
    n_threads=4,
    verbose=False
)

def llm_generate(prompt: str, max_tokens=256) -> str:
    output = LLM(
        prompt,
        max_tokens=max_tokens,
        stop=["\n", "###", "User:"],
        echo=False
    )
    return output["choices"][0]["text"].strip()
```

> ğŸ’¡ æç¤ºï¼šä½ å¯ä»¥å°† `llm_generate` å°è£…ä¸ºå¯é…ç½®å‡½æ•°ï¼Œæ”¯æŒä¸åŒæ¨¡å‹/å‚æ•°ã€‚

---

## 6ï¸âƒ£ ç¤ºä¾‹ Agentic Graph

ğŸ“ æ–‡ä»¶ï¼š`examples/weather_agent.yaml`

```yaml
graph:
  name: "Weather Agent"
  version: "1.0"

nodes:
  - id: start
    type: start
    next: set_query

  - id: set_query
    type: set
    assign:
      user_request: "What's the weather in Beijing today?"
      query: "weather in Beijing"
    next: search

  - id: search
    type: tool_call
    tool: web_search
    args:
      query: "{{query}}"
    output_key: search_result
    next: summarize

  - id: summarize
    type: llm_call
    prompt_template: "Summarize this search result in one sentence: {{search_result}}"
    output_key: final_answer
    next: end

  - id: end
    type: end
```

---

## 7ï¸âƒ£ è¿è¡Œè„šæœ¬

ğŸ“ æ–‡ä»¶ï¼š`run_example.py`

```python
import yaml
import logging
from core.engine import ExecutionEngine
from tools import TOOLS
from llm.llama_client import llm_generate

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

def main():
    with open("examples/weather_agent.yaml") as f:
        graph = yaml.safe_load(f)

    engine = ExecutionEngine(
        graph_def=graph,
        tools=TOOLS,
        llm_client=llm_generate
    )
    engine.run()

    print("\nâœ… Final context:")
    print(engine.context)

if __name__ == "__main__":
    main()
```

---

## â–¶ï¸ å¦‚ä½•è¿è¡Œ

```bash
# å®‰è£…ä¾èµ–
pip install pyyaml jinja2 llama-cpp-python

# ä¿®æ”¹ llama_client.py ä¸­çš„ model_path ä¸ºä½ æœ¬åœ°çš„ .gguf æ¨¡å‹è·¯å¾„

# è¿è¡Œç¤ºä¾‹
python run_example.py
```

---

## âœ… é˜¶æ®µ 1 éªŒè¯ç‚¹

- [ ] YAML èƒ½è¢«æ­£ç¡®åŠ è½½
- [ ] `set` èŠ‚ç‚¹æ­£ç¡®è®¾ç½®å˜é‡
- [ ] `tool_call` è°ƒç”¨ mock å·¥å…·å¹¶å­˜å…¥ context
- [ ] `llm_call` è°ƒç”¨æœ¬åœ° LLM å¹¶è¿”å›ç»“æœ
- [ ] æ‰§è¡ŒæŒ‰ `next` é¡ºåºæµè½¬
- [ ] æ—¥å¿—æ¸…æ™°æ˜¾ç¤ºæ¯æ­¥æ“ä½œ

---

## ğŸš€ ä¸‹ä¸€æ­¥

ä¸€æ—¦ä½ æˆåŠŸè¿è¡Œé˜¶æ®µ 1ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š
1. æ·»åŠ  **JSON Schema éªŒè¯**ï¼ˆä½¿ç”¨ `jsonschema` åº“ï¼‰
2. å¢å¼º **è¡¨è¾¾å¼æ”¯æŒ**ï¼ˆå¦‚ `{{ len(results) > 0 }}`ï¼‰
3. è¿›å…¥ **é˜¶æ®µ 2ï¼šæ¡ä»¶åˆ†æ”¯ä¸å¾ªç¯**

è¯·å°è¯•è¿è¡Œï¼Œå¹¶å‘Šè¯‰æˆ‘æ˜¯å¦é‡åˆ°é—®é¢˜ï¼ˆå°¤å…¶æ˜¯ llama.cpp æ¨¡å‹åŠ è½½éƒ¨åˆ†ï¼‰ã€‚æˆ‘å¯ä»¥å¸®ä½ è°ƒæ•´ LLM é€‚é…å™¨ã€‚
