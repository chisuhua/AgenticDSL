#include "agenticdsl/core/engine.h"
#include "agenticdsl/llm/prompt_builder.h"
#include <iostream>
#include <fstream>
#include <sstream>

// åŠ è½½åˆå§‹ DSL æ–‡ä»¶
std::string load_file(const std::string& path) {
    std::ifstream file(path);
    if (!file.is_open()) {
        throw std::runtime_error("Cannot open file: " + path);
    }
    std::stringstream buffer;
    buffer << file.rdbuf();
    return buffer.str();
}

// æ„å»ºåŒ…å«å†å²å’Œå¯ç”¨åº“çš„ prompt
std::string build_prompt(const agenticdsl::Context& ctx, const std::string& paused_at) {
    // ä½¿ç”¨ Inja æ¨¡æ¿æ„å»ºæ›´ç»“æ„åŒ–çš„ prompt
    std::string base_prompt = R"(
You are an AI agent that generates AgenticDSL code to continue the workflow.

Current context:
{{ context | dump(2) }}

You are paused at node: {{ paused_at }}

Available standard libraries:
{% for lib in available_subgraphs %}
- Path: {{ lib.path }}
  {% if lib.signature %}Signature: {{ lib.signature }}{% endif %}
  Permissions: {{ lib.permissions | join(", ") or "none" }}
{% endfor %}

Generate ONLY the next AgenticDSL block(s) in the exact format below. Do NOT explain.

### AgenticDSL `/main/stepX`
```yaml
# --- BEGIN AgenticDSL ---
type: ...
...
# --- END AgenticDSL ---
```
)";

    agenticdsl::Context prompt_ctx;
    prompt_ctx["context"] = ctx;
    prompt_ctx["paused_at"] = paused_at;

    return agenticdsl::PromptBuilder::inject_libraries_into_prompt(base_prompt, prompt_ctx);
}

int main() {
    try {
        // 1. åŠ è½½åˆå§‹ DSLï¼ˆå¿…é¡»åŒ…å« /main å­å›¾ï¼‰
        auto engine = agenticdsl::DSLEngine::from_file("initial.md");

        // 2. åˆå§‹åŒ–ä¸Šä¸‹æ–‡
        agenticdsl::Context ctx;
        ctx["user_input"] = "Calculate 15 + 27 and then get the weather in Beijing.";
        ctx["history"] = nlohmann::json::array();

        int max_steps = 5; // é˜²æ­¢æ— é™å¾ªç¯
        int step = 0;

        while (step < max_steps) {
            std::cout << "\n--- Agent Step " << (step + 1) << " ---\n";

            // 3. æ‰§è¡Œå½“å‰ DAG
            auto result = engine->run(ctx);
            ctx = result.final_context;

            if (!result.success) {
                std::cerr << "âŒ Execution failed: " << result.message << "\n";
                break;
            }

            // 4. æ£€æŸ¥æ˜¯å¦æš‚åœï¼ˆLLM_CALL èŠ‚ç‚¹ï¼‰
            if (result.paused_at) {
                std::cout << "â¸ï¸  Paused at: " << *result.paused_at << "\n";

                // 5. æ„å»º promptï¼ˆå«å¯ç”¨æ ‡å‡†åº“ï¼‰
                std::string prompt = build_prompt(ctx, *result.paused_at);
                std::cout << "ğŸ“ Prompt:\n" << prompt << "\n";

                // 6. è°ƒç”¨ LLM ç”Ÿæˆæ–° DSL
                std::string new_dsl = engine->get_llm_adapter()->generate(prompt);
                std::cout << "ğŸ¤– LLM Generated DSL:\n" << new_dsl << "\n";

                // 7. è®°å½•å†å²
                nlohmann::json history_entry;
                history_entry["prompt"] = prompt;
                history_entry["generated_dsl"] = new_dsl;
                ctx["history"].push_back(history_entry);

                // 8. ã€å…³é”®ã€‘ä½¿ç”¨ continue_with_generated_dsl è§£æå¹¶åˆå¹¶
                engine->continue_with_generated_dsl(new_dsl);

                std::cout << "âœ… Appended new blocks. Continuing...\n";
                step++;
            } else {
                std::cout << "âœ… Workflow completed successfully.\n";
                std::cout << "Final context:\n" << ctx.dump(2) << "\n";
                break;
            }
        }

        if (step >= max_steps) {
            std::cout << "âš ï¸  Max steps reached. Terminating.\n";
        }

    } catch (const std::exception& e) {
        std::cerr << "ğŸ’¥ Fatal error: " << e.what() << "\n";
        return 1;
    }

    return 0;
}
